{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import collections\n",
    "import nltk\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from textblob import TextBlob\n",
    "from statistics import mean\n",
    "from nltk.probability import FreqDist\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/abhishekvaidyanathan/Desktop/NLP-project1/reviewSelected100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       0      0     0   \n",
       "1      4       0      0     0   \n",
       "2      5       2      1     3   \n",
       "3      3       0      0     0   \n",
       "4      1      11      0     3   \n",
       "\n",
       "                                                text                 date  \n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['business_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    filtered_sentence = []\n",
    "    for w in tokenized_sentence:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(sentence):\n",
    "    word_tokens_with_stop = word_tokenize(sentence)\n",
    "    word_tokens_with_stop = [word for word in word_tokens_with_stop if word. isalpha()]\n",
    "\n",
    "    # word_tokens_with_stop = [word for word in word_tokens_with_stop if word.lower()!=\"i\"]\n",
    "    word_tokens=remove_stopwords(word_tokens_with_stop)\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet lemmatizer\n",
    "def wordnet_lemmatizer(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(w) for w in tokenised_sentence]\n",
    "    return(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter stemming\n",
    "def Porter_stemming(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    porter = PorterStemmer()\n",
    "    Porter_stemming_sentence = [porter.stem(w) for w in tokenised_sentence]\n",
    "    return(Porter_stemming_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster stemming\n",
    "def Lancaster_stemming(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    lancaster=LancasterStemmer()\n",
    "    Lancaster_stemming_sentence = [lancaster.stem(w) for w in tokenised_sentence]\n",
    "    return(Lancaster_stemming_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random business id\n",
    "def random_business_id(df):\n",
    "    n = random.randint(0,len(df.business_id)) \n",
    "    return df.business_id[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_review_extracter(dataset,business_id_to_check):\n",
    "    reviews_text=[]\n",
    "    reviews_sentences=[]\n",
    "    reviews_tokens=[]\n",
    "    for i in range(0,len(dataset.business_id)):\n",
    "        if dataset.business_id[i]==business_id_to_check:\n",
    "            reviews_text.append(dataset.text[i])\n",
    "    for i in range(len(reviews_text)):\n",
    "        reviews_sentences.append(nltk.tokenize.sent_tokenize(reviews_text[i]))\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            reviews_tokens.append(tokenisation(reviews_sentences[i][j]))\n",
    "    return reviews_text,reviews_sentences,reviews_tokens\n",
    "#display word frequency distribution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_lancaster(reviews_sentences):\n",
    "    lancaster_stemmed=[]\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            lancaster_stemmed.append(Lancaster_stemming(reviews_sentences[i][j]))\n",
    "    return lancaster_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_poter(reviews_sentences):\n",
    "    Porter_stemmed=[]\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            Porter_stemmed.append(Porter_stemming(reviews_sentences[i][j]))\n",
    "    return Porter_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_wordnet(reviews_sentences):\n",
    "    wordnet_lemmatized=[]\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            wordnet_lemmatized.append(wordnet_lemmatizer(reviews_sentences[i][j]))\n",
    "    return wordnet_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tagging(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    Pos_Tag_Sentence=nltk.pos_tag(tokenised_sentence)\n",
    "    return Pos_Tag_Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(sentence):\n",
    "    tokenised_sentence = []\n",
    "    tokenised_sentence = sent_tokenize(sentence)\n",
    "    return tokenised_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentence_array):\n",
    "    sentence_tokens = []\n",
    "    for sentences in sentence_array:\n",
    "        tokenize_sent = nltk_pos_tagging(sentences)\n",
    "        sentence_tokens.append(tokenize_sent)\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_sentiment(lyrics):\n",
    "  analysis = TextBlob(lyrics)\n",
    "  return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_sentiment(sentence_array):\n",
    "    sentences_sentiment = []\n",
    "    for sentences in sentence_array:\n",
    "        sentiment = get_text_sentiment(sentences)\n",
    "        sentences_sentiment.append(sentiment)\n",
    "    return sentences_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_adjective_pairs(sentence_array,sentiment_array):\n",
    "    count_array = []\n",
    "    i = 0\n",
    "    k = 0\n",
    "    for pos_tags_array in sentence_array:\n",
    "        pos_tags_count_array = []\n",
    "        for pos_tags in range(len(pos_tags_array)):\n",
    "            if (((pos_tags_array[pos_tags][1]) == \"NN\") or ((pos_tags_array[pos_tags][1]) == \"NNS\") or ((pos_tags_array[pos_tags][1]) == \"NNP\") or ((pos_tags_array[pos_tags][1]) == \"NNPS\")):\n",
    "                i = pos_tags+1\n",
    "                j = pos_tags-1\n",
    "                while i <len(pos_tags_array) and j>=0:\n",
    "                    if(pos_tags_array[j][1]==\"JJ\"):\n",
    "                        adjective = wordnet_lemmatizer(pos_tags_array[j][0])[0].lower()\n",
    "                        noun = wordnet_lemmatizer(pos_tags_array[pos_tags][0])[0].lower()\n",
    "                        pos_tags_count_array.append([noun,adjective])\n",
    "                        break\n",
    "                    elif(pos_tags_array[i][1]==\"JJ\"):\n",
    "                        adjective = wordnet_lemmatizer(pos_tags_array[i][0])[0].lower()\n",
    "                        noun = wordnet_lemmatizer(pos_tags_array[pos_tags][0])[0].lower()\n",
    "                        pos_tags_count_array.append([noun,adjective])\n",
    "                        break\n",
    "                    if(i<len(pos_tags_array)):\n",
    "                        i = i+1\n",
    "                    if(j-1>=0):\n",
    "                        j = j-1\n",
    "        count_array.append([pos_tags_count_array,len(pos_tags_count_array),sentiment_array[k]])\n",
    "        k = k+1\n",
    "        i = i+1\n",
    "\n",
    "    return count_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_noun_adjective_pairs(noun_adjective_pairs,all_noun_adjective_pairs,all_noun_adjective_sentiments):\n",
    "    for all_pairs in noun_adjective_pairs:\n",
    "        if (all_pairs[0]!=[]):\n",
    "            all_noun_adjective_pairs.extend(all_pairs[0])\n",
    "            for i in range(all_pairs[1]):\n",
    "                all_noun_adjective_sentiments.append(all_pairs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_tuple(all_noun_adjective_pairs):\n",
    "    for i in range(len(all_noun_adjective_pairs)):\n",
    "        all_noun_adjective_pairs[i] = tuple(all_noun_adjective_pairs[i])\n",
    "\n",
    "    return all_noun_adjective_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_word_pair(all_noun_adjective_pairs_tuple,top_k,indexes):\n",
    "    counter=collections.Counter(all_noun_adjective_pairs_tuple)\n",
    "    dict1 = dict(counter)\n",
    "    final_list = counter.most_common(top_k)\n",
    "    for i in range(len(all_noun_adjective_pairs_tuple)):\n",
    "        for j in final_list:\n",
    "            if(all_noun_adjective_pairs_tuple[i]==j[0]):\n",
    "                indexes.append(i)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_for_specific_word_pairs(all_noun_adjective_pairs_tuple,key_value):\n",
    "    counter=collections.Counter(all_noun_adjective_pairs_tuple)\n",
    "    dict1 = dict(counter)\n",
    "    return dict1[key_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_and_common_word_pairs(tuple1,tuple2):\n",
    "    list1_as_set = set(tuple1)\n",
    "    intersection = list1_as_set.intersection(tuple2)\n",
    "    intersection_as_list = list(intersection)\n",
    "    for common_elements in intersection_as_list:\n",
    "        count1 = get_count_for_specific_word_pairs(tuple1,common_elements)\n",
    "        count2 = get_count_for_specific_word_pairs(tuple2,common_elements)\n",
    "        print(\"The common element is \",common_elements,\". The count from from each of the tuples are: \",str([count1,count2]),\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_frequent_pairs_words(data_rating,top_k):\n",
    "    all_noun_adjective_pairs_rating = []\n",
    "    all_noun_adjective_sentiments = []\n",
    "    dummy_sentiments = []\n",
    "    mean_sentiments = []\n",
    "    indexes = []\n",
    "    data_rating.apply(lambda row: extract_all_noun_adjective_pairs(row['noun_adjective_pairs'],all_noun_adjective_pairs_rating,all_noun_adjective_sentiments),axis=1)\n",
    "    all_noun_adjective_pairs__rating_tuple = convert_list_to_tuple(all_noun_adjective_pairs_rating)\n",
    "    top_frequent_pairs_rating = [get_most_common_word_pair(all_noun_adjective_pairs__rating_tuple,top_k,indexes)]\n",
    "    # top_frequent_pairs_rating.append(indexes)\n",
    "    i = 0\n",
    "    for index in indexes:\n",
    "        dummy_sentiments.append(all_noun_adjective_sentiments[index])\n",
    "\n",
    "    for pairs in top_frequent_pairs_rating[0]:\n",
    "        mean_sentiments.append(mean(dummy_sentiments[i:i+pairs[1]]))\n",
    "        i = i+pairs[1]\n",
    "    # top_frequent_pairs_rating.append(all_noun_adjective_sentiments)\n",
    "    top_frequent_pairs_rating.append(mean_sentiments)\n",
    "\n",
    "    return (top_frequent_pairs_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_for_the_frequent_word_pairs(data,top_k):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    frequency = []\n",
    "    word_pairs = []\n",
    "    for i in get_top_frequent_pairs_words(data,top_k)[0]:\n",
    "        frequency.append(int(i[1]))\n",
    "        word_pairs.append(str(i[0]))\n",
    "    ax.bar(tuple(word_pairs),tuple(frequency))\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_xlabel('Word Pairs')\n",
    "    ax.set_title('frequence of top '+str(top_k)+' word pairs')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_tags(sentence_array):\n",
    "    ner_tags = []\n",
    "    # print(sentence_array['Sentence'])\n",
    "    for sentence in sentence_array:\n",
    "        # print(sentence)\n",
    "        ner  = nlp(sentence)\n",
    "        ner_copy = ner.copy()\n",
    "        # ner_array = []\n",
    "        i = 1\n",
    "        while i < len(ner_copy):\n",
    "            if(ner[i]['word'][:2]==\"##\" and ner[i-1]['entity']==ner[i]['entity']):\n",
    "                ner_copy[i-1]['word'] = ner[i-1]['word'] + ner[i]['word'][2:]\n",
    "                ner_copy[i-1]['score'] = (ner[i-1]['score'] + ner[i]['score'])/2\n",
    "                ner_copy[i-1]['entity'] = ner[i-1]['entity']\n",
    "                ner_copy[i-1]['index'] = ner[i-1]['index']\n",
    "                ner_copy[i-1]['start'] = ner[i-1]['start']\n",
    "                ner_copy[i-1]['end'] = ner[i]['end']\n",
    "                ner_copy.remove(ner_copy[i])\n",
    "                continue\n",
    "            i = i+1\n",
    "        ner_tags.append(ner_copy)\n",
    "    return ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>vY6UNbjwOT1eo7xc1ZgF2g</td>\n",
       "      <td>9GJ6XOBFBcokyG4GnVB4AQ</td>\n",
       "      <td>sj9osyqLyOy7b_kDZb1txA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Had high hopes after reading online reviews th...</td>\n",
       "      <td>2015-03-20 17:47:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ny5uHRwJhVMc69aCT2y5wA</td>\n",
       "      <td>vRl2e5TmB3tSwPtfyMYnuw</td>\n",
       "      <td>RyaCGkXRXxXNeJhbnioz1Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hope you like dregs, because that's all you'll...</td>\n",
       "      <td>2015-12-22 20:17:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>jcbHPpwJLP4uADiRLKOidA</td>\n",
       "      <td>JUbShoeYLmk76Q1n9yv9wQ</td>\n",
       "      <td>WA7sC64kCRstywm2EgZXEw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I had heard really good things about this rest...</td>\n",
       "      <td>2018-09-02 17:49:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3GawFvNuqR1gIKYdSbsJiQ</td>\n",
       "      <td>URb0hVQv5jMuktO9odV83A</td>\n",
       "      <td>XA_m9daZl2VFDA6alnkBvg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Not impressed. I sampled the Tuna and it was p...</td>\n",
       "      <td>2017-08-18 00:43:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>cw_Z8y5J8ACeBX0kNQH4Rw</td>\n",
       "      <td>KgLN_fu-baMQkVCodvp9xw</td>\n",
       "      <td>vMpJzMFst_9GP4boeqWIRg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>found hair in my sweet and sour chicken. enoug...</td>\n",
       "      <td>2016-06-15 20:46:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  review_id                 user_id             business_id  \\\n",
       "124  vY6UNbjwOT1eo7xc1ZgF2g  9GJ6XOBFBcokyG4GnVB4AQ  sj9osyqLyOy7b_kDZb1txA   \n",
       "59   ny5uHRwJhVMc69aCT2y5wA  vRl2e5TmB3tSwPtfyMYnuw  RyaCGkXRXxXNeJhbnioz1Q   \n",
       "66   jcbHPpwJLP4uADiRLKOidA  JUbShoeYLmk76Q1n9yv9wQ  WA7sC64kCRstywm2EgZXEw   \n",
       "68   3GawFvNuqR1gIKYdSbsJiQ  URb0hVQv5jMuktO9odV83A  XA_m9daZl2VFDA6alnkBvg   \n",
       "130  cw_Z8y5J8ACeBX0kNQH4Rw  KgLN_fu-baMQkVCodvp9xw  vMpJzMFst_9GP4boeqWIRg   \n",
       "\n",
       "     stars  useful  funny  cool  \\\n",
       "124      1       0      0     0   \n",
       "59       1       0      1     0   \n",
       "66       1       0      0     0   \n",
       "68       1       0      0     0   \n",
       "130      1       0      0     0   \n",
       "\n",
       "                                                  text                 date  \n",
       "124  Had high hopes after reading online reviews th...  2015-03-20 17:47:13  \n",
       "59   Hope you like dregs, because that's all you'll...  2015-12-22 20:17:27  \n",
       "66   I had heard really good things about this rest...  2018-09-02 17:49:51  \n",
       "68   Not impressed. I sampled the Tuna and it was p...  2017-08-18 00:43:46  \n",
       "130  found hair in my sweet and sour chicken. enoug...  2016-06-15 20:46:07  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rating_1 = data[data['stars']==1]\n",
    "data_random_new = data_rating_1.groupby('business_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "data_random_50_rating_1 = data_random_new.sample(50)\n",
    "data_random_50_rating_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_noun_adjective_pairs(data):\n",
    "    data = data.copy()\n",
    "    print(\"processing pos tags\")\n",
    "    data['pos_tags'] = data.apply(lambda row: nltk_pos_tagging(row['text']),axis=1)\n",
    "    # print(\"processing text sentiments\")\n",
    "    # data['text_sentiment'] = data.apply(lambda row: get_text_sentiment(row['text']), axis=1)\n",
    "    # print(\"processing sentence tokenizer\")\n",
    "    # data['sentence tokenizer'] = data.apply(lambda row: sentence_tokenizer(row['text']),axis=1)\n",
    "    print(\"processing text sentiment\")\n",
    "    data['text_sentiment'] = data.apply(lambda row: get_text_sentiment(row['text']), axis=1)\n",
    "    print(\"processing sentence tokenizer\")\n",
    "    data['sentence tokenizer'] = data.apply(lambda row: sentence_tokenizer(row['text']),axis=1)\n",
    "    print(\"processing sentence pos tags\")\n",
    "    data[\"sentence_tokens_pos_tags\"] = data.apply(lambda row: tokenize_sentences(row['sentence tokenizer']),axis=1)\n",
    "    print(\"processing sentencs sentence\")\n",
    "    data['sentences_sentiment'] = data.apply(lambda row: get_sentence_sentiment(row['sentence tokenizer']),axis=1)\n",
    "    print(\"processing noun adjective pairs\")\n",
    "    data[\"noun_adjective_pairs\"] = data.apply(lambda row: count_noun_adjective_pairs(row['sentence_tokens_pos_tags'],row['sentences_sentiment']),axis=1)\n",
    "    # print(\"processing bert ner tags\")\n",
    "    # data['bert_ner'] = data.apply(lambda row: get_ner_tags(row['sentence tokenizer']),axis=1)\n",
    "    print('processing completed')\n",
    "    # top_frequent = get_top_frequent_pairs_words(data,3)\n",
    "    # print(\"processing done\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pos tags\n",
      "processing text sentiment\n",
      "processing sentence tokenizer\n",
      "processing sentence pos tags\n",
      "processing sentencs sentence\n",
      "processing noun adjective pairs\n",
      "processing completed\n"
     ]
    }
   ],
   "source": [
    "data_noun_adjective_pairs = get_final_noun_adjective_pairs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequent = get_top_frequent_pairs_words(data_noun_adjective_pairs,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(('time', 'first'), 615),\n",
       "  (('food', 'good'), 470),\n",
       "  (('place', 'great'), 380),\n",
       "  (('food', 'great'), 302),\n",
       "  (('service', 'great'), 274),\n",
       "  (('service', 'good'), 257),\n",
       "  (('place', 'good'), 255),\n",
       "  (('time', 'next'), 247),\n",
       "  (('time', 'last'), 231),\n",
       "  (('hour', 'happy'), 172)],\n",
       " [0.368934837396971,\n",
       "  0.38311848279773814,\n",
       "  0.4053332217516193,\n",
       "  0.41749379579914114,\n",
       "  0.4189002272607428,\n",
       "  0.39679488846051053,\n",
       "  0.40746215915788747,\n",
       "  0.3629659766851595,\n",
       "  0.35484455419520355,\n",
       "  0.3900579170809004]]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': ',', 'score': 0.7667328715324402, 'entity': 'I-ORG', 'index': 27, 'start': 90, 'end': 91}, {'word': 'Music', 'score': 0.8218851685523987, 'entity': 'I-ORG', 'index': 28, 'start': 92, 'end': 97}, {'word': 'And', 'score': 0.9378843903541565, 'entity': 'I-ORG', 'index': 29, 'start': 98, 'end': 101}, {'word': 'Wait', 'score': 0.9378998875617981, 'entity': 'I-ORG', 'index': 30, 'start': 102, 'end': 106}, {'word': '##ers', 'score': 0.6082433462142944, 'entity': 'I-ORG', 'index': 31, 'start': 106, 'end': 109}, {'word': 'Lyle', 'score': 0.6591613292694092, 'entity': 'B-PER', 'index': 38, 'start': 131, 'end': 135}]\n"
     ]
    }
   ],
   "source": [
    "ner  = nlp(data.iloc[0]['text'].title())\n",
    "if len(ner) != 0:\n",
    "    print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word', 'score', 'entity', 'index', 'start', 'end'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_word = nlp('Shale Williams'.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'S',\n",
       "  'score': 0.9993637204170227,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'word': '##hale',\n",
       "  'score': 0.99187833070755,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 2,\n",
       "  'start': 1,\n",
       "  'end': 5},\n",
       " {'word': 'Williams',\n",
       "  'score': 0.997721791267395,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 3,\n",
       "  'start': 6,\n",
       "  'end': 14}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Wolfgang', 'score': 0.9994356632232666, 'entity': 'B-PER', 'index': 4, 'start': 11, 'end': 19}, {'word': 'S', 'score': 0.9989044666290283, 'entity': 'I-PER', 'index': 5, 'start': 20, 'end': 21}, {'word': '##hale', 'score': 0.9992238879203796, 'entity': 'I-PER', 'index': 6, 'start': 21, 'end': 25}, {'word': 'Williams', 'score': 0.9986526966094971, 'entity': 'I-PER', 'index': 7, 'start': 26, 'end': 34}, {'word': 'Berlin', 'score': 0.9995962977409363, 'entity': 'B-LOC', 'index': 12, 'start': 49, 'end': 55}]\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Wolfgang Shale Williams and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun_adjective_pairs['bert_ner'] = data_noun_adjective_pairs.apply(lambda row: get_ner_tags(row['sentence tokenizer']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id                                 8aoJJdKEO3ypoZNszpPu7Q\n",
       "user_id                                   bGgAL09pxLnV_FFgR4ZADg\n",
       "business_id                               ZBE-H_aUlicix_9vUGQPIQ\n",
       "stars                                                          5\n",
       "useful                                                         0\n",
       "funny                                                          0\n",
       "cool                                                           0\n",
       "text           We had my Mother's Birthday Party here on 10/2...\n",
       "date                                         2016-11-09 20:07:25\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[{'word': 'Birthday', 'score': 0.8212296366691589, 'entity': 'I-MISC', 'index': 7, 'start': 19, 'end': 27}, {'word': 'Party', 'score': 0.6545494198799133, 'entity': 'I-MISC', 'index': 8, 'start': 28, 'end': 33}], [], [], [{'word': 'Lyle', 'score': 0.8679577708244324, 'entity': 'B-PER', 'index': 2, 'start': 7, 'end': 11}], []]\n",
      "1\n",
      "[[{'word': 'Korean', 'score': 0.9950916767120361, 'entity': 'B-MISC', 'index': 2, 'start': 5, 'end': 11}, {'word': 'Eaton', 'score': 0.995835542678833, 'entity': 'B-LOC', 'index': 6, 'start': 23, 'end': 28}, {'word': 'Centre', 'score': 0.9984296560287476, 'entity': 'I-LOC', 'index': 7, 'start': 29, 'end': 35}], [], [], [], [], [], [], [], [], [], [{'word': 'B', 'score': 0.554015576839447, 'entity': 'B-MISC', 'index': 8, 'start': 25, 'end': 26}]]\n",
      "2\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [{'word': 'Kealiali', 'score': 0.6973328590393066, 'entity': 'B-PER', 'index': 10, 'start': 37, 'end': 42}], [{'word': 'Ke', 'score': 0.9983730912208557, 'entity': 'B-PER', 'index': 7, 'start': 28, 'end': 30}, {'word': '##ali', 'score': 0.49731892347335815, 'entity': 'I-PER', 'index': 8, 'start': 30, 'end': 33}, {'word': \"'\", 'score': 0.7349721193313599, 'entity': 'I-PER', 'index': 9, 'start': 33, 'end': 34}, {'word': 'i', 'score': 0.41960278153419495, 'entity': 'I-PER', 'index': 10, 'start': 34, 'end': 35}, {'word': 'Reichel', 'score': 0.7853466272354126, 'entity': 'I-PER', 'index': 11, 'start': 36, 'end': 43}], [], []]\n",
      "3\n",
      "[[{'word': 'Cannerynery', 'score': 0.5049330219626427, 'entity': 'B-LOC', 'index': 12, 'start': 50, 'end': 57}], [], [], [{'word': 'Vegas', 'score': 0.9987011551856995, 'entity': 'B-LOC', 'index': 24, 'start': 111, 'end': 116}], [], [], [], [], [{'word': 'St', 'score': 0.9708152413368225, 'entity': 'B-PER', 'index': 1, 'start': 0, 'end': 2}], [], [{'word': 'Cal', 'score': 0.9971663355827332, 'entity': 'B-ORG', 'index': 1, 'start': 0, 'end': 3}, {'word': '##amari', 'score': 0.9760619103908539, 'entity': 'I-ORG', 'index': 2, 'start': 3, 'end': 8}], [], []]\n",
      "4\n",
      "[[], [], [], [], [{'word': 'SH', 'score': 0.4599020779132843, 'entity': 'B-ORG', 'index': 26, 'start': 109, 'end': 111}], [{'word': 'B', 'score': 0.9940023422241211, 'entity': 'B-ORG', 'index': 25, 'start': 111, 'end': 112}, {'word': '##BB', 'score': 0.893062174320221, 'entity': 'I-ORG', 'index': 26, 'start': 112, 'end': 114}], [], [], [{'word': 'Amazon', 'score': 0.9990777969360352, 'entity': 'B-LOC', 'index': 28, 'start': 114, 'end': 120}], [], []]\n",
      "5\n",
      "[[{'word': 'Ka', 'score': 0.7260799407958984, 'entity': 'B-MISC', 'index': 5, 'start': 18, 'end': 20}]]\n",
      "6\n",
      "[[], [], [], []]\n",
      "7\n",
      "[[], [], [], [{'word': 'BB', 'score': 0.6661299467086792, 'entity': 'B-MISC', 'index': 4, 'start': 15, 'end': 17}], []]\n",
      "8\n",
      "[[], [], [], []]\n",
      "9\n",
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "for row in range(0,10):\n",
    "    print(row)\n",
    "    print(get_ner_tags(data_noun_adjective_pairs.iloc[row]['sentence tokenizer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun_adjective_pairs.to_csv('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/fullDataNounAdjectivePairs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews = data.groupby(['business_id'], as_index = False).agg({'text': '. '.join})\n",
    "combined_reviews.text = combined_reviews.text.apply(lambda x: x.replace('\\n', ''))\n",
    "combined_reviews.text = combined_reviews.text.apply(lambda x: x.replace('\\r', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...\n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...\n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...\n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...\n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of..."
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_lsa(text):\n",
    "    parser=PlaintextParser.from_string(text,Tokenizer('english'))\n",
    "    lsa_summarizer=LsaSummarizer()\n",
    "    lsa_summary = lsa_summarizer(parser.document,100)\n",
    "    summary = []\n",
    "    for i in lsa_summary:\n",
    "        summary.append(str(i))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews['summary'] = combined_reviews.apply(lambda row: summarizer_lsa(row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \n",
       "0  [Had to get my wing fix, I like dry rubs on wi...  \n",
       "1  [One time, we arrived around 5 on a weekday ho...  \n",
       "2  [However this location is the dirtiest I have ...  \n",
       "3  [Also, a killer name for someone who works in ...  \n",
       "4  [Different vibe all together and I found the m...  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews['bert_ner'] = combined_reviews.apply(lambda row: get_ner_tags(row['summary']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>bert_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "      <td>[[], [], [], [], [], [], [], [], [{'word': 'Ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "      <td>[[], [], [{'word': 'Was', 'score': 0.471216082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "      <td>[[], [], [], [{'word': 'L', 'score': 0.5113500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "      <td>[[], [], [], [], [], [{'word': 'DD', 'score': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "      <td>[[{'word': 'Val', 'score': 0.7798781991004944,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  [Had to get my wing fix, I like dry rubs on wi...   \n",
       "1  [One time, we arrived around 5 on a weekday ho...   \n",
       "2  [However this location is the dirtiest I have ...   \n",
       "3  [Also, a killer name for someone who works in ...   \n",
       "4  [Different vibe all together and I found the m...   \n",
       "\n",
       "                                            bert_ner  \n",
       "0  [[], [], [], [], [], [], [], [], [{'word': 'Ap...  \n",
       "1  [[], [], [{'word': 'Was', 'score': 0.471216082...  \n",
       "2  [[], [], [], [{'word': 'L', 'score': 0.5113500...  \n",
       "3  [[], [], [], [], [], [{'word': 'DD', 'score': ...  \n",
       "4  [[{'word': 'Val', 'score': 0.7798781991004944,...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews.to_csv('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/combined_reviews_bert_ner.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(combined_reviews.shape[0]):\n",
    "    ner_tags = combined_reviews.iloc[i]['bert_ner']\n",
    "    while [] in ner_tags:\n",
    "        ner_tags.remove([])\n",
    "    combined_reviews.at[row,'bert_ner']= ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>bert_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "      <td>[[{'word': 'Apple', 'score': 0.651425600051879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "      <td>[[{'word': 'Was', 'score': 0.471216082572937, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "      <td>[[{'word': 'L', 'score': 0.5113500952720642, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "      <td>[[{'word': 'DD', 'score': 0.529673159122467, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "      <td>[[{'word': 'Val', 'score': 0.7798781991004944,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  [Had to get my wing fix, I like dry rubs on wi...   \n",
       "1  [One time, we arrived around 5 on a weekday ho...   \n",
       "2  [However this location is the dirtiest I have ...   \n",
       "3  [Also, a killer name for someone who works in ...   \n",
       "4  [Different vibe all together and I found the m...   \n",
       "\n",
       "                                            bert_ner  \n",
       "0  [[{'word': 'Apple', 'score': 0.651425600051879...  \n",
       "1  [[{'word': 'Was', 'score': 0.471216082572937, ...  \n",
       "2  [[{'word': 'L', 'score': 0.5113500952720642, '...  \n",
       "3  [[{'word': 'DD', 'score': 0.529673159122467, '...  \n",
       "4  [[{'word': 'Val', 'score': 0.7798781991004944,...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_id = {}\n",
    "for i in range(combined_reviews.shape[0]):\n",
    "    word = []\n",
    "    for j in range(len(combined_reviews.iloc[i]['bert_ner'])):\n",
    "        for k in range(len(combined_reviews.iloc[i]['bert_ner'][j])):\n",
    "            word.append(combined_reviews.iloc[i]['bert_ner'][j][k]['word'])\n",
    "    \n",
    "    business_id[combined_reviews.iloc[i]['business_id']] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "for business in business_id:\n",
    "    j = 0\n",
    "    while j < len(business_id[business]):\n",
    "        if(j>0 and business_id[business][j][:2] == '##'):\n",
    "            business_id[business][j-1] = business_id[business][j-1] + business_id[business][j][2:]\n",
    "            business_id[business].remove(business_id[business][j])\n",
    "            continue\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bf/k_40pbd90j3401sv0sntrcgw0000gn/T/ipykernel_44379/651364812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mreview_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0mreview_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3381\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             result = self._constructor_sliced(\n\u001b[0m\u001b[1;32m   3384\u001b[0m                 \u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.data_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"block\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleArrayManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfrom_array\u001b[0;34m(cls, array, index)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0mConstructor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myet\u001b[0m \u001b[0ma\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \"\"\"\n\u001b[0;32m-> 1567\u001b[0;31m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "business_review_id = {}\n",
    "for business in business_id:\n",
    "    word_dict = {}\n",
    "    for word in business_id[business]:\n",
    "        review_id = []\n",
    "        for i in range(data.shape[0]):\n",
    "            if(word in data.iloc[i]['text'].split()):\n",
    "                review_id.append(data.iloc[i]['review_id'])\n",
    "        word_dict[word] = review_id\n",
    "    business_id[business] = word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_review_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "with open('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/business_review_id.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(business_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

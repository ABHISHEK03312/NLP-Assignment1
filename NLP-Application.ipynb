{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import collections\n",
    "import nltk\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from textblob import TextBlob\n",
    "from statistics import mean\n",
    "from nltk.probability import FreqDist\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/data/dataset/reviewSelected100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      5       0      0     0   \n",
       "1      4       0      0     0   \n",
       "2      5       2      1     3   \n",
       "3      3       0      0     0   \n",
       "4      1      11      0     3   \n",
       "\n",
       "                                                text                 date  \n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['business_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    filtered_sentence = []\n",
    "    for w in tokenized_sentence:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(sentence):\n",
    "    word_tokens_with_stop = word_tokenize(sentence)\n",
    "    word_tokens_with_stop = [word for word in word_tokens_with_stop if word. isalpha()]\n",
    "\n",
    "    # word_tokens_with_stop = [word for word in word_tokens_with_stop if word.lower()!=\"i\"]\n",
    "    word_tokens=remove_stopwords(word_tokens_with_stop)\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tagging(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    Pos_Tag_Sentence=nltk.pos_tag(tokenised_sentence)\n",
    "    return Pos_Tag_Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(sentence):\n",
    "    tokenised_sentence = []\n",
    "    tokenised_sentence = sent_tokenize(sentence)\n",
    "    return tokenised_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentence_array):\n",
    "    sentence_tokens = []\n",
    "    for sentences in sentence_array:\n",
    "        tokenize_sent = nltk_pos_tagging(sentences)\n",
    "        sentence_tokens.append(tokenize_sent)\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_tags(sentence_array):\n",
    "    ner_tags = []\n",
    "    # print(sentence_array['Sentence'])\n",
    "    for sentence in sentence_array:\n",
    "        # print(sentence)\n",
    "        ner  = nlp(sentence)\n",
    "        ner_copy = ner.copy()\n",
    "        # ner_array = []\n",
    "        i = 1\n",
    "        while i < len(ner_copy):\n",
    "            if(ner[i]['word'][:2]==\"##\" and ner[i-1]['entity']==ner[i]['entity']):\n",
    "                ner_copy[i-1]['word'] = ner[i-1]['word'] + ner[i]['word'][2:]\n",
    "                ner_copy[i-1]['score'] = (ner[i-1]['score'] + ner[i]['score'])/2\n",
    "                ner_copy[i-1]['entity'] = ner[i-1]['entity']\n",
    "                ner_copy[i-1]['index'] = ner[i-1]['index']\n",
    "                ner_copy[i-1]['start'] = ner[i-1]['start']\n",
    "                ner_copy[i-1]['end'] = ner[i]['end']\n",
    "                ner_copy.remove(ner_copy[i])\n",
    "                continue\n",
    "            i = i+1\n",
    "        ner_tags.append(ner_copy)\n",
    "    return ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>vY6UNbjwOT1eo7xc1ZgF2g</td>\n",
       "      <td>9GJ6XOBFBcokyG4GnVB4AQ</td>\n",
       "      <td>sj9osyqLyOy7b_kDZb1txA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Had high hopes after reading online reviews th...</td>\n",
       "      <td>2015-03-20 17:47:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ny5uHRwJhVMc69aCT2y5wA</td>\n",
       "      <td>vRl2e5TmB3tSwPtfyMYnuw</td>\n",
       "      <td>RyaCGkXRXxXNeJhbnioz1Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hope you like dregs, because that's all you'll...</td>\n",
       "      <td>2015-12-22 20:17:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>jcbHPpwJLP4uADiRLKOidA</td>\n",
       "      <td>JUbShoeYLmk76Q1n9yv9wQ</td>\n",
       "      <td>WA7sC64kCRstywm2EgZXEw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I had heard really good things about this rest...</td>\n",
       "      <td>2018-09-02 17:49:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3GawFvNuqR1gIKYdSbsJiQ</td>\n",
       "      <td>URb0hVQv5jMuktO9odV83A</td>\n",
       "      <td>XA_m9daZl2VFDA6alnkBvg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Not impressed. I sampled the Tuna and it was p...</td>\n",
       "      <td>2017-08-18 00:43:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>cw_Z8y5J8ACeBX0kNQH4Rw</td>\n",
       "      <td>KgLN_fu-baMQkVCodvp9xw</td>\n",
       "      <td>vMpJzMFst_9GP4boeqWIRg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>found hair in my sweet and sour chicken. enoug...</td>\n",
       "      <td>2016-06-15 20:46:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  review_id                 user_id             business_id  \\\n",
       "124  vY6UNbjwOT1eo7xc1ZgF2g  9GJ6XOBFBcokyG4GnVB4AQ  sj9osyqLyOy7b_kDZb1txA   \n",
       "59   ny5uHRwJhVMc69aCT2y5wA  vRl2e5TmB3tSwPtfyMYnuw  RyaCGkXRXxXNeJhbnioz1Q   \n",
       "66   jcbHPpwJLP4uADiRLKOidA  JUbShoeYLmk76Q1n9yv9wQ  WA7sC64kCRstywm2EgZXEw   \n",
       "68   3GawFvNuqR1gIKYdSbsJiQ  URb0hVQv5jMuktO9odV83A  XA_m9daZl2VFDA6alnkBvg   \n",
       "130  cw_Z8y5J8ACeBX0kNQH4Rw  KgLN_fu-baMQkVCodvp9xw  vMpJzMFst_9GP4boeqWIRg   \n",
       "\n",
       "     stars  useful  funny  cool  \\\n",
       "124      1       0      0     0   \n",
       "59       1       0      1     0   \n",
       "66       1       0      0     0   \n",
       "68       1       0      0     0   \n",
       "130      1       0      0     0   \n",
       "\n",
       "                                                  text                 date  \n",
       "124  Had high hopes after reading online reviews th...  2015-03-20 17:47:13  \n",
       "59   Hope you like dregs, because that's all you'll...  2015-12-22 20:17:27  \n",
       "66   I had heard really good things about this rest...  2018-09-02 17:49:51  \n",
       "68   Not impressed. I sampled the Tuna and it was p...  2017-08-18 00:43:46  \n",
       "130  found hair in my sweet and sour chicken. enoug...  2016-06-15 20:46:07  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rating_1 = data[data['stars']==1]\n",
    "data_random_new = data_rating_1.groupby('business_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "data_random_50_rating_1 = data_random_new.sample(50)\n",
    "data_random_50_rating_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_noun_adjective_pairs(data):\n",
    "    data = data.copy()\n",
    "    print(\"processing pos tags\")\n",
    "    data['pos_tags'] = data.apply(lambda row: nltk_pos_tagging(row['text']),axis=1)\n",
    "    # print(\"processing text sentiments\")\n",
    "    # data['text_sentiment'] = data.apply(lambda row: get_text_sentiment(row['text']), axis=1)\n",
    "    # print(\"processing sentence tokenizer\")\n",
    "    # data['sentence tokenizer'] = data.apply(lambda row: sentence_tokenizer(row['text']),axis=1)\n",
    "    print(\"processing text sentiment\")\n",
    "    data['text_sentiment'] = data.apply(lambda row: get_text_sentiment(row['text']), axis=1)\n",
    "    print(\"processing sentence tokenizer\")\n",
    "    data['sentence tokenizer'] = data.apply(lambda row: sentence_tokenizer(row['text']),axis=1)\n",
    "    print(\"processing sentence pos tags\")\n",
    "    data[\"sentence_tokens_pos_tags\"] = data.apply(lambda row: tokenize_sentences(row['sentence tokenizer']),axis=1)\n",
    "    print(\"processing sentencs sentence\")\n",
    "    data['sentences_sentiment'] = data.apply(lambda row: get_sentence_sentiment(row['sentence tokenizer']),axis=1)\n",
    "    print(\"processing noun adjective pairs\")\n",
    "    data[\"noun_adjective_pairs\"] = data.apply(lambda row: count_noun_adjective_pairs(row['sentence_tokens_pos_tags'],row['sentences_sentiment']),axis=1)\n",
    "    # print(\"processing bert ner tags\")\n",
    "    # data['bert_ner'] = data.apply(lambda row: get_ner_tags(row['sentence tokenizer']),axis=1)\n",
    "    print('processing completed')\n",
    "    # top_frequent = get_top_frequent_pairs_words(data,3)\n",
    "    # print(\"processing done\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pos tags\n",
      "processing text sentiment\n",
      "processing sentence tokenizer\n",
      "processing sentence pos tags\n",
      "processing sentencs sentence\n",
      "processing noun adjective pairs\n",
      "processing completed\n"
     ]
    }
   ],
   "source": [
    "data_noun_adjective_pairs = get_final_noun_adjective_pairs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequent = get_top_frequent_pairs_words(data_noun_adjective_pairs,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(('time', 'first'), 615),\n",
       "  (('food', 'good'), 470),\n",
       "  (('place', 'great'), 380),\n",
       "  (('food', 'great'), 302),\n",
       "  (('service', 'great'), 274),\n",
       "  (('service', 'good'), 257),\n",
       "  (('place', 'good'), 255),\n",
       "  (('time', 'next'), 247),\n",
       "  (('time', 'last'), 231),\n",
       "  (('hour', 'happy'), 172)],\n",
       " [0.368934837396971,\n",
       "  0.38311848279773814,\n",
       "  0.4053332217516193,\n",
       "  0.41749379579914114,\n",
       "  0.4189002272607428,\n",
       "  0.39679488846051053,\n",
       "  0.40746215915788747,\n",
       "  0.3629659766851595,\n",
       "  0.35484455419520355,\n",
       "  0.3900579170809004]]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': ',', 'score': 0.7667328715324402, 'entity': 'I-ORG', 'index': 27, 'start': 90, 'end': 91}, {'word': 'Music', 'score': 0.8218851685523987, 'entity': 'I-ORG', 'index': 28, 'start': 92, 'end': 97}, {'word': 'And', 'score': 0.9378843903541565, 'entity': 'I-ORG', 'index': 29, 'start': 98, 'end': 101}, {'word': 'Wait', 'score': 0.9378998875617981, 'entity': 'I-ORG', 'index': 30, 'start': 102, 'end': 106}, {'word': '##ers', 'score': 0.6082433462142944, 'entity': 'I-ORG', 'index': 31, 'start': 106, 'end': 109}, {'word': 'Lyle', 'score': 0.6591613292694092, 'entity': 'B-PER', 'index': 38, 'start': 131, 'end': 135}]\n"
     ]
    }
   ],
   "source": [
    "ner  = nlp(data.iloc[0]['text'].title())\n",
    "if len(ner) != 0:\n",
    "    print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word', 'score', 'entity', 'index', 'start', 'end'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_word = nlp('Shale Williams'.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'S',\n",
       "  'score': 0.9993637204170227,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'word': '##hale',\n",
       "  'score': 0.99187833070755,\n",
       "  'entity': 'B-PER',\n",
       "  'index': 2,\n",
       "  'start': 1,\n",
       "  'end': 5},\n",
       " {'word': 'Williams',\n",
       "  'score': 0.997721791267395,\n",
       "  'entity': 'I-PER',\n",
       "  'index': 3,\n",
       "  'start': 6,\n",
       "  'end': 14}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Wolfgang', 'score': 0.9994356632232666, 'entity': 'B-PER', 'index': 4, 'start': 11, 'end': 19}, {'word': 'S', 'score': 0.9989044666290283, 'entity': 'I-PER', 'index': 5, 'start': 20, 'end': 21}, {'word': '##hale', 'score': 0.9992238879203796, 'entity': 'I-PER', 'index': 6, 'start': 21, 'end': 25}, {'word': 'Williams', 'score': 0.9986526966094971, 'entity': 'I-PER', 'index': 7, 'start': 26, 'end': 34}, {'word': 'Berlin', 'score': 0.9995962977409363, 'entity': 'B-LOC', 'index': 12, 'start': 49, 'end': 55}]\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Wolfgang Shale Williams and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun_adjective_pairs['bert_ner'] = data_noun_adjective_pairs.apply(lambda row: get_ner_tags(row['sentence tokenizer']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id                                 8aoJJdKEO3ypoZNszpPu7Q\n",
       "user_id                                   bGgAL09pxLnV_FFgR4ZADg\n",
       "business_id                               ZBE-H_aUlicix_9vUGQPIQ\n",
       "stars                                                          5\n",
       "useful                                                         0\n",
       "funny                                                          0\n",
       "cool                                                           0\n",
       "text           We had my Mother's Birthday Party here on 10/2...\n",
       "date                                         2016-11-09 20:07:25\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[{'word': 'Birthday', 'score': 0.8212296366691589, 'entity': 'I-MISC', 'index': 7, 'start': 19, 'end': 27}, {'word': 'Party', 'score': 0.6545494198799133, 'entity': 'I-MISC', 'index': 8, 'start': 28, 'end': 33}], [], [], [{'word': 'Lyle', 'score': 0.8679577708244324, 'entity': 'B-PER', 'index': 2, 'start': 7, 'end': 11}], []]\n",
      "1\n",
      "[[{'word': 'Korean', 'score': 0.9950916767120361, 'entity': 'B-MISC', 'index': 2, 'start': 5, 'end': 11}, {'word': 'Eaton', 'score': 0.995835542678833, 'entity': 'B-LOC', 'index': 6, 'start': 23, 'end': 28}, {'word': 'Centre', 'score': 0.9984296560287476, 'entity': 'I-LOC', 'index': 7, 'start': 29, 'end': 35}], [], [], [], [], [], [], [], [], [], [{'word': 'B', 'score': 0.554015576839447, 'entity': 'B-MISC', 'index': 8, 'start': 25, 'end': 26}]]\n",
      "2\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [{'word': 'Kealiali', 'score': 0.6973328590393066, 'entity': 'B-PER', 'index': 10, 'start': 37, 'end': 42}], [{'word': 'Ke', 'score': 0.9983730912208557, 'entity': 'B-PER', 'index': 7, 'start': 28, 'end': 30}, {'word': '##ali', 'score': 0.49731892347335815, 'entity': 'I-PER', 'index': 8, 'start': 30, 'end': 33}, {'word': \"'\", 'score': 0.7349721193313599, 'entity': 'I-PER', 'index': 9, 'start': 33, 'end': 34}, {'word': 'i', 'score': 0.41960278153419495, 'entity': 'I-PER', 'index': 10, 'start': 34, 'end': 35}, {'word': 'Reichel', 'score': 0.7853466272354126, 'entity': 'I-PER', 'index': 11, 'start': 36, 'end': 43}], [], []]\n",
      "3\n",
      "[[{'word': 'Cannerynery', 'score': 0.5049330219626427, 'entity': 'B-LOC', 'index': 12, 'start': 50, 'end': 57}], [], [], [{'word': 'Vegas', 'score': 0.9987011551856995, 'entity': 'B-LOC', 'index': 24, 'start': 111, 'end': 116}], [], [], [], [], [{'word': 'St', 'score': 0.9708152413368225, 'entity': 'B-PER', 'index': 1, 'start': 0, 'end': 2}], [], [{'word': 'Cal', 'score': 0.9971663355827332, 'entity': 'B-ORG', 'index': 1, 'start': 0, 'end': 3}, {'word': '##amari', 'score': 0.9760619103908539, 'entity': 'I-ORG', 'index': 2, 'start': 3, 'end': 8}], [], []]\n",
      "4\n",
      "[[], [], [], [], [{'word': 'SH', 'score': 0.4599020779132843, 'entity': 'B-ORG', 'index': 26, 'start': 109, 'end': 111}], [{'word': 'B', 'score': 0.9940023422241211, 'entity': 'B-ORG', 'index': 25, 'start': 111, 'end': 112}, {'word': '##BB', 'score': 0.893062174320221, 'entity': 'I-ORG', 'index': 26, 'start': 112, 'end': 114}], [], [], [{'word': 'Amazon', 'score': 0.9990777969360352, 'entity': 'B-LOC', 'index': 28, 'start': 114, 'end': 120}], [], []]\n",
      "5\n",
      "[[{'word': 'Ka', 'score': 0.7260799407958984, 'entity': 'B-MISC', 'index': 5, 'start': 18, 'end': 20}]]\n",
      "6\n",
      "[[], [], [], []]\n",
      "7\n",
      "[[], [], [], [{'word': 'BB', 'score': 0.6661299467086792, 'entity': 'B-MISC', 'index': 4, 'start': 15, 'end': 17}], []]\n",
      "8\n",
      "[[], [], [], []]\n",
      "9\n",
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "for row in range(0,10):\n",
    "    print(row)\n",
    "    print(get_ner_tags(data_noun_adjective_pairs.iloc[row]['sentence tokenizer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun_adjective_pairs.to_csv('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/fullDataNounAdjectivePairs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews = data.groupby(['business_id'], as_index = False).agg({'text': '. '.join})\n",
    "combined_reviews.text = combined_reviews.text.apply(lambda x: x.replace('\\n', ''))\n",
    "combined_reviews.text = combined_reviews.text.apply(lambda x: x.replace('\\r', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...\n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...\n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...\n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...\n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of..."
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_lsa(text):\n",
    "    parser=PlaintextParser.from_string(text,Tokenizer('english'))\n",
    "    lsa_summarizer=LsaSummarizer()\n",
    "    lsa_summary = lsa_summarizer(parser.document,100)\n",
    "    summary = []\n",
    "    for i in lsa_summary:\n",
    "        summary.append(str(i))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews['summary'] = combined_reviews.apply(lambda row: summarizer_lsa(row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \n",
       "0  [Had to get my wing fix, I like dry rubs on wi...  \n",
       "1  [One time, we arrived around 5 on a weekday ho...  \n",
       "2  [However this location is the dirtiest I have ...  \n",
       "3  [Also, a killer name for someone who works in ...  \n",
       "4  [Different vibe all together and I found the m...  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews['bert_ner'] = combined_reviews.apply(lambda row: get_ner_tags(row['summary']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>bert_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "      <td>[[], [], [], [], [], [], [], [], [{'word': 'Ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "      <td>[[], [], [{'word': 'Was', 'score': 0.471216082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "      <td>[[], [], [], [{'word': 'L', 'score': 0.5113500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "      <td>[[], [], [], [], [], [{'word': 'DD', 'score': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "      <td>[[{'word': 'Val', 'score': 0.7798781991004944,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  [Had to get my wing fix, I like dry rubs on wi...   \n",
       "1  [One time, we arrived around 5 on a weekday ho...   \n",
       "2  [However this location is the dirtiest I have ...   \n",
       "3  [Also, a killer name for someone who works in ...   \n",
       "4  [Different vibe all together and I found the m...   \n",
       "\n",
       "                                            bert_ner  \n",
       "0  [[], [], [], [], [], [], [], [], [{'word': 'Ap...  \n",
       "1  [[], [], [{'word': 'Was', 'score': 0.471216082...  \n",
       "2  [[], [], [], [{'word': 'L', 'score': 0.5113500...  \n",
       "3  [[], [], [], [], [], [{'word': 'DD', 'score': ...  \n",
       "4  [[{'word': 'Val', 'score': 0.7798781991004944,...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews.to_csv('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/combined_reviews_bert_ner.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(combined_reviews.shape[0]):\n",
    "    ner_tags = combined_reviews.iloc[i]['bert_ner']\n",
    "    while [] in ner_tags:\n",
    "        ner_tags.remove([])\n",
    "    combined_reviews.at[row,'bert_ner']= ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>bert_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "      <td>[[{'word': 'Apple', 'score': 0.651425600051879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "      <td>[[{'word': 'Was', 'score': 0.471216082572937, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "      <td>[[{'word': 'L', 'score': 0.5113500952720642, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "      <td>[[{'word': 'DD', 'score': 0.529673159122467, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "      <td>[[{'word': 'Val', 'score': 0.7798781991004944,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  [Had to get my wing fix, I like dry rubs on wi...   \n",
       "1  [One time, we arrived around 5 on a weekday ho...   \n",
       "2  [However this location is the dirtiest I have ...   \n",
       "3  [Also, a killer name for someone who works in ...   \n",
       "4  [Different vibe all together and I found the m...   \n",
       "\n",
       "                                            bert_ner  \n",
       "0  [[{'word': 'Apple', 'score': 0.651425600051879...  \n",
       "1  [[{'word': 'Was', 'score': 0.471216082572937, ...  \n",
       "2  [[{'word': 'L', 'score': 0.5113500952720642, '...  \n",
       "3  [[{'word': 'DD', 'score': 0.529673159122467, '...  \n",
       "4  [[{'word': 'Val', 'score': 0.7798781991004944,...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_id = {}\n",
    "for i in range(combined_reviews.shape[0]):\n",
    "    word = []\n",
    "    for j in range(len(combined_reviews.iloc[i]['bert_ner'])):\n",
    "        for k in range(len(combined_reviews.iloc[i]['bert_ner'][j])):\n",
    "            word.append(combined_reviews.iloc[i]['bert_ner'][j][k]['word'])\n",
    "    \n",
    "    business_id[combined_reviews.iloc[i]['business_id']] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "for business in business_id:\n",
    "    j = 0\n",
    "    while j < len(business_id[business]):\n",
    "        if(j>0 and business_id[business][j][:2] == '##'):\n",
    "            business_id[business][j-1] = business_id[business][j-1] + business_id[business][j][2:]\n",
    "            business_id[business].remove(business_id[business][j])\n",
    "            continue\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_id = {}\n",
    "for business in business_id:\n",
    "    word_dict = {}\n",
    "    for word in business_id[business]:\n",
    "        review_id = []\n",
    "        for i in range(data.shape[0]):\n",
    "            if(word in data.iloc[i]['text'].split()):\n",
    "                review_id.append(data.iloc[i]['review_id'])\n",
    "        word_dict[word] = review_id\n",
    "    business_review_id[business] = word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "with open('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/business_review_id_new.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(business_review_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/business_review_id_new.json', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(business_review_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_id_stopwords = business_id.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    filtered_sentence = []\n",
    "    for w in tokenized_sentence:\n",
    "        if (w.lower() not in stop_words ) and (w.lower().isalpha()) and (len(w)>1):\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_key_value_pair(key):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    if (key.lower() not in stop_words ) and (key.lower().isalpha()) and (len(key)>1):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in business_id_stopwords:\n",
    "    business_id_stopwords[key] = remove_stopwords(business_id_stopwords[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TGIff'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F  = \"TGIff\"\n",
    "re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_id_copy = business_review_id.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_ner_tags(business_review_id_copy):\n",
    "    business_review_id_clean = {}\n",
    "    for key in business_review_id_copy:\n",
    "        business_review_id_clean[key] = {}\n",
    "        for keys in business_review_id_copy[key]:\n",
    "            keys_new = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", keys)\n",
    "            if(remove_key_value_pair(keys_new) and business_review_id_copy[key][keys] != []):\n",
    "                business_review_id_clean[key][keys_new] = business_review_id_copy[key][keys] \n",
    "    return business_review_id_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_id_clean = get_cleaned_ner_tags(business_review_id_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/business_review_id_clean.json', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(business_review_id_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>bert_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "      <td>[[{'word': 'Apple', 'score': 0.651425600051879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "      <td>[[{'word': 'Was', 'score': 0.471216082572937, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "      <td>[[{'word': 'L', 'score': 0.5113500952720642, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "      <td>[[{'word': 'DD', 'score': 0.529673159122467, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "      <td>[[{'word': 'Val', 'score': 0.7798781991004944,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>yHHVKa9joZAKiBDUp2SkKw</td>\n",
       "      <td>This coffee shop has great coffee, but more im...</td>\n",
       "      <td>[I have been here a total of two times and fou...</td>\n",
       "      <td>[[{'word': 'Mesa', 'score': 0.9974473118782043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>z8Em-bhZI3Mmspml7tj6tg</td>\n",
       "      <td>This was the first time and probably the last ...</td>\n",
       "      <td>[I gave them two stars because of the free sof...</td>\n",
       "      <td>[[{'word': 'B', 'score': 0.9204431176185608, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>zPEYgVqJ2QNKi45FJi2jvg</td>\n",
       "      <td>Stopped by here for lunch yesterday.  What a d...</td>\n",
       "      <td>[My husband got the chicken and waffles which ...</td>\n",
       "      <td>[[{'word': 'World', 'score': 0.998627364635467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>zZ7KDK3GAkBUZzsaqB1A4Q</td>\n",
       "      <td>Staff is SO friendly, I was checked in on once...</td>\n",
       "      <td>[Quick service with good food, nothing but gre...</td>\n",
       "      <td>[[{'word': 'Pittsburgh', 'score': 0.9980792999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>z_RTK0MaxaRHRQ99hDL2QA</td>\n",
       "      <td>So so good. I am a big fan of the NY style piz...</td>\n",
       "      <td>[They are like the equivalent of two regular N...</td>\n",
       "      <td>[[{'word': 'NY', 'score': 0.9757542014122009, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id  \\\n",
       "0    --I7YYLada0tSLkORTHb5Q   \n",
       "1    -7XWJYkutqhIxLen7Grg1g   \n",
       "2    0Rni7ocMC_Lg2UH0lDeKMQ   \n",
       "3    0kPm1zEpeXFRg8D2phqgCQ   \n",
       "4    1Fpk8ibHhZYnCw8fnGny8w   \n",
       "..                      ...   \n",
       "148  yHHVKa9joZAKiBDUp2SkKw   \n",
       "149  z8Em-bhZI3Mmspml7tj6tg   \n",
       "150  zPEYgVqJ2QNKi45FJi2jvg   \n",
       "151  zZ7KDK3GAkBUZzsaqB1A4Q   \n",
       "152  z_RTK0MaxaRHRQ99hDL2QA   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Had to get my wing fix, I like dry rubs on win...   \n",
       "1    Definite recommend. But I never would have kno...   \n",
       "2    We love Barros!! Usually go to other locations...   \n",
       "3    Coffee is exponentially better than Starbucks,...   \n",
       "4    Really love the food here! I was a HUGE fan of...   \n",
       "..                                                 ...   \n",
       "148  This coffee shop has great coffee, but more im...   \n",
       "149  This was the first time and probably the last ...   \n",
       "150  Stopped by here for lunch yesterday.  What a d...   \n",
       "151  Staff is SO friendly, I was checked in on once...   \n",
       "152  So so good. I am a big fan of the NY style piz...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    [Had to get my wing fix, I like dry rubs on wi...   \n",
       "1    [One time, we arrived around 5 on a weekday ho...   \n",
       "2    [However this location is the dirtiest I have ...   \n",
       "3    [Also, a killer name for someone who works in ...   \n",
       "4    [Different vibe all together and I found the m...   \n",
       "..                                                 ...   \n",
       "148  [I have been here a total of two times and fou...   \n",
       "149  [I gave them two stars because of the free sof...   \n",
       "150  [My husband got the chicken and waffles which ...   \n",
       "151  [Quick service with good food, nothing but gre...   \n",
       "152  [They are like the equivalent of two regular N...   \n",
       "\n",
       "                                              bert_ner  \n",
       "0    [[{'word': 'Apple', 'score': 0.651425600051879...  \n",
       "1    [[{'word': 'Was', 'score': 0.471216082572937, ...  \n",
       "2    [[{'word': 'L', 'score': 0.5113500952720642, '...  \n",
       "3    [[{'word': 'DD', 'score': 0.529673159122467, '...  \n",
       "4    [[{'word': 'Val', 'score': 0.7798781991004944,...  \n",
       "..                                                 ...  \n",
       "148  [[{'word': 'Mesa', 'score': 0.9974473118782043...  \n",
       "149  [[{'word': 'B', 'score': 0.9204431176185608, '...  \n",
       "150  [[{'word': 'World', 'score': 0.998627364635467...  \n",
       "151  [[{'word': 'Pittsburgh', 'score': 0.9980792999...  \n",
       "152  [[{'word': 'NY', 'score': 0.9757542014122009, ...  \n",
       "\n",
       "[153 rows x 4 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_nouns(text_array):\n",
    "    text = ' '.join(text_array)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # nouns_and_verbs = [token[0] for token in tagged_tokens if token[1] in ['NN','NNP','NNS','NNPS']]\n",
    "    nouns_and_verbs = []\n",
    "    for token in tagged_tokens:\n",
    "        if(token[1] in ['NN','NNP','NNS','NNPS'] and token[0].isalpha()):\n",
    "            nouns_and_verbs.append(token[0])\n",
    "    frequency = nltk.FreqDist(nouns_and_verbs)\n",
    "    # frequency.most_common(10)\n",
    "    return frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reviews['most_common_nouns'] = combined_reviews.apply(lambda row: get_most_common_nouns(row['summary']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>bert_ner</th>\n",
       "      <th>most_common_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
       "      <td>Had to get my wing fix, I like dry rubs on win...</td>\n",
       "      <td>[Had to get my wing fix, I like dry rubs on wi...</td>\n",
       "      <td>[[{'word': 'Apple', 'score': 0.651425600051879...</td>\n",
       "      <td>[(place, 16), (food, 11), (beer, 7), (bar, 7),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7XWJYkutqhIxLen7Grg1g</td>\n",
       "      <td>Definite recommend. But I never would have kno...</td>\n",
       "      <td>[One time, we arrived around 5 on a weekday ho...</td>\n",
       "      <td>[[{'word': 'Was', 'score': 0.471216082572937, ...</td>\n",
       "      <td>[(sushi, 19), (AYCE, 17), (place, 15), (food, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>We love Barros!! Usually go to other locations...</td>\n",
       "      <td>[However this location is the dirtiest I have ...</td>\n",
       "      <td>[[{'word': 'L', 'score': 0.5113500952720642, '...</td>\n",
       "      <td>[(pizza, 33), (order, 15), (wings, 8), (kids, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0kPm1zEpeXFRg8D2phqgCQ</td>\n",
       "      <td>Coffee is exponentially better than Starbucks,...</td>\n",
       "      <td>[Also, a killer name for someone who works in ...</td>\n",
       "      <td>[[{'word': 'DD', 'score': 0.529673159122467, '...</td>\n",
       "      <td>[(coffee, 20), (donuts, 13), (order, 11), (ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Fpk8ibHhZYnCw8fnGny8w</td>\n",
       "      <td>Really love the food here! I was a HUGE fan of...</td>\n",
       "      <td>[Different vibe all together and I found the m...</td>\n",
       "      <td>[[{'word': 'Val', 'score': 0.7798781991004944,...</td>\n",
       "      <td>[(place, 13), (night, 9), (people, 9), (rice, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text  \\\n",
       "0  --I7YYLada0tSLkORTHb5Q  Had to get my wing fix, I like dry rubs on win...   \n",
       "1  -7XWJYkutqhIxLen7Grg1g  Definite recommend. But I never would have kno...   \n",
       "2  0Rni7ocMC_Lg2UH0lDeKMQ  We love Barros!! Usually go to other locations...   \n",
       "3  0kPm1zEpeXFRg8D2phqgCQ  Coffee is exponentially better than Starbucks,...   \n",
       "4  1Fpk8ibHhZYnCw8fnGny8w  Really love the food here! I was a HUGE fan of...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  [Had to get my wing fix, I like dry rubs on wi...   \n",
       "1  [One time, we arrived around 5 on a weekday ho...   \n",
       "2  [However this location is the dirtiest I have ...   \n",
       "3  [Also, a killer name for someone who works in ...   \n",
       "4  [Different vibe all together and I found the m...   \n",
       "\n",
       "                                            bert_ner  \\\n",
       "0  [[{'word': 'Apple', 'score': 0.651425600051879...   \n",
       "1  [[{'word': 'Was', 'score': 0.471216082572937, ...   \n",
       "2  [[{'word': 'L', 'score': 0.5113500952720642, '...   \n",
       "3  [[{'word': 'DD', 'score': 0.529673159122467, '...   \n",
       "4  [[{'word': 'Val', 'score': 0.7798781991004944,...   \n",
       "\n",
       "                                   most_common_nouns  \n",
       "0  [(place, 16), (food, 11), (beer, 7), (bar, 7),...  \n",
       "1  [(sushi, 19), (AYCE, 17), (place, 15), (food, ...  \n",
       "2  [(pizza, 33), (order, 15), (wings, 8), (kids, ...  \n",
       "3  [(coffee, 20), (donuts, 13), (order, 11), (ser...  \n",
       "4  [(place, 13), (night, 9), (people, 9), (rice, ...  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_value_values = [combined_reviews.iloc[0]['most_common_nouns'],combined_reviews.iloc[1]['most_common_nouns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('place', 16),\n",
       "  ('food', 11),\n",
       "  ('beer', 7),\n",
       "  ('bar', 7),\n",
       "  ('night', 6),\n",
       "  ('server', 6),\n",
       "  ('service', 6),\n",
       "  ('wings', 5),\n",
       "  ('tables', 5),\n",
       "  ('family', 5)],\n",
       " [('sushi', 19),\n",
       "  ('AYCE', 17),\n",
       "  ('place', 15),\n",
       "  ('food', 14),\n",
       "  ('service', 13),\n",
       "  ('places', 12),\n",
       "  ('restaurant', 10),\n",
       "  ('people', 8),\n",
       "  ('Mississauga', 8),\n",
       "  ('eat', 8)]]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_value_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_noun = {}\n",
    "# for i in range(combined_reviews.shape[0]):\n",
    "#     word_dict_noun = {}\n",
    "#     for j in range(combined_reviews.iloc[i]['most_common_nouns']):\n",
    "#         review_id_noun = []\n",
    "#         for k in range(data.shape[0]):\n",
    "#             if(combined_reviews.iloc[j]['most_common_nouns'][0] in data.iloc[k]['text'].split()):\n",
    "#                 review_id_noun.append(data.iloc[k]['review_id'])\n",
    "#         word_dict_noun[combined_reviews.iloc[j]['most_common_nouns'][0] ] = review_id\n",
    "#     business_review_noun[combined_reviews.iloc[i]['business_id']] = word_dict_noun\n",
    "\n",
    "# business_review_id = {}\n",
    "# for business in business_id:\n",
    "#     word_dict = {}\n",
    "#     for word in business_id[business]:\n",
    "#         review_id = []\n",
    "#         for i in range(data.shape[0]):\n",
    "#             if(word in data.iloc[i]['text'].split()):\n",
    "#                 review_id.append(data.iloc[i]['review_id'])\n",
    "#         word_dict[word] = review_id\n",
    "#     business_review_id[business] = word_dict\n",
    "\n",
    "for i in range(combined_reviews.shape[0]):\n",
    "    word_dict_noun = {}\n",
    "    for j in combined_reviews.iloc[i]['most_common_nouns']:\n",
    "        review_id_noun = []\n",
    "        for k in range(data.shape[0]):\n",
    "            if(j[0] in data.iloc[k]['text'].split()):\n",
    "                review_id_noun.append(data.iloc[k]['review_id'])\n",
    "        word_dict_noun[j[0]] = review_id_noun\n",
    "    business_review_noun[combined_reviews.iloc[i]['business_id']] = word_dict_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/abhishekvaidyanathan/Desktop/NLP-Assignment1/data/application_ner_tags/business_review_noun.json', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(business_review_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

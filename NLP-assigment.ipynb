{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import random\n",
    "import re\n",
    "sb.set() # set the default Seaborn style for graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset [ Need to change the location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/abhi/Downloads/reviewSelected100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words from the Tokenised sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in tokenized_sentence if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in tokenized_sentence:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(sentence):\n",
    "    word_tokens_with_stop = word_tokenize(sentence)\n",
    "    word_tokens=remove_stopwords(word_tokens_with_stop)\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization functions of sentence\n",
    "### 1. Wordnet lemmatizer using nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_lemmatizer(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(w) for w in tokenised_sentence]\n",
    "    return(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Google Bert package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming functions of sentence\n",
    "### 1. Porter Stemmer using nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Porter_stemming(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    porter = PorterStemmer()\n",
    "    Porter_stemming_sentence = [porter.stem(w) for w in tokenised_sentence]\n",
    "    return(Porter_stemming_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lancaster Stemmer using nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lancaster_stemming(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    lancaster=LancasterStemmer()\n",
    "    Lancaster_stemming_sentence = [lancaster.stem(w) for w in tokenised_sentence]\n",
    "    return(Lancaster_stemming_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Google Bert package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging functions of sentence\n",
    "### 1. Using nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tagging(sentence):\n",
    "    tokenised_sentence=tokenisation(sentence)\n",
    "    Pos_Tag_Sentence=nltk.pos_tag(tokenised_sentence)\n",
    "    return Pos_Tag_Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using google bert package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_business_id(df):\n",
    "    n = random.randint(0,len(df.business_id)) \n",
    "    return df.business_id[n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_review_extracter(business_id_to_check):\n",
    "    reviews_text=[]\n",
    "    reviews_sentences=[]\n",
    "    reviews_tokens=[]\n",
    "    for i in range(0,len(dataset.business_id)):\n",
    "        if dataset.business_id[i]==business_id_to_check:\n",
    "            reviews_text.append(dataset.text[i])\n",
    "    for i in range(len(reviews_text)):\n",
    "        reviews_sentences.append(nltk.tokenize.sent_tokenize(reviews_text[i]))\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            reviews_tokens.append(tokenisation(reviews_sentences[i][j]))\n",
    "    return reviews_text,reviews_sentences,reviews_tokens\n",
    "#display word frequency distribution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_lancaster(reviews_sentences):\n",
    "    lancaster_stemmed=[]\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            lancaster_stemmed.append(Lancaster_stemming(reviews_sentences[i][j]))\n",
    "    return lancaster_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_poter(reviews_sentences):\n",
    "    Porter_stemmed=[]\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            Porter_stemmed.append(Porter_stemming(reviews_sentences[i][j]))\n",
    "    return Porter_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_wordnet(reviews_sentences):\n",
    "    wordnet_lemmatized=[]\n",
    "    for i in range(len(reviews_sentences)):\n",
    "        for j in range(len(reviews_sentences[i])):\n",
    "            wordnet_lemmatized.append(wordnet_lemmatizer(reviews_sentences[i][j]))\n",
    "    return wordnet_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Dataset Analysis\n",
    "### a) Tokenisation, Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1=random_business_id(dataset)\n",
    "reviews_B1_text,reviews_B1_sentences,reviews_B1_tokens = business_review_extracter(B1)\n",
    "#display word frequency distribution here\n",
    "Porter_Stemming_B1=business_poter(reviews_B1_sentences)\n",
    "Lancaster_Stemming_B1=business_lancaster(reviews_B1_sentences)\n",
    "Wordnet_lematization_B1=business_wordnet(reviews_B1_sentences)\n",
    "#display word frequency distribution here\n",
    "B2=random_business_id(dataset)\n",
    "while(B1!=B2):\n",
    "    B2=random_business_id(dataset)\n",
    "reviews_B2_text,reviews_B2_sentences,reviews_B2_tokens = business_review_extracter(B2)\n",
    "#display word frequency distribution here\n",
    "Porter_Stemming_B1=business_poter(reviews_B2_sentences)\n",
    "Lancaster_Stemming_B1=business_lancaster(reviews_B2_sentences)\n",
    "Wordnet_lematization_B2=business_wordnet(reviews_B2_sentences)\n",
    "#display word frequency distribution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I got the usual Kabob but tried fish from my mother wish I has ordered that!', 'Let me start by saying how much I hate Las Vegas but am forced to attend an annual conference there.', 'This place is disorganized.', 'The place is nothing to look at, but the tacos are delicious!', 'I was happy to see this in Terminal 4 pre-security and have been eating here for years.']\n",
      "[[('I', 'PRP'), ('got', 'VBD'), ('usual', 'JJ'), ('Kabob', 'NNP'), ('tried', 'VBD'), ('fish', 'JJ'), ('mother', 'NN'), ('wish', 'JJ'), ('I', 'PRP'), ('ordered', 'VBD'), ('!', '.')], [('Let', 'VB'), ('start', 'VB'), ('saying', 'VBG'), ('much', 'JJ'), ('I', 'PRP'), ('hate', 'VBP'), ('Las', 'NNP'), ('Vegas', 'NNP'), ('forced', 'VBD'), ('attend', 'JJ'), ('annual', 'JJ'), ('conference', 'NN'), ('.', '.')], [('This', 'DT'), ('place', 'NN'), ('disorganized', 'VBD'), ('.', '.')], [('The', 'DT'), ('place', 'NN'), ('nothing', 'NN'), ('look', 'NN'), (',', ','), ('tacos', 'RB'), ('delicious', 'JJ'), ('!', '.')], [('I', 'PRP'), ('happy', 'JJ'), ('see', 'VBP'), ('Terminal', 'JJ'), ('4', 'CD'), ('pre-security', 'JJ'), ('eating', 'JJ'), ('years', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "n=random.sample(range(0, len(dataset.text)), 5)\n",
    "target=[]\n",
    "reviews_tokens=[]\n",
    "reviews_sentences=[]\n",
    "required_sentences=[]\n",
    "for i in range(0,5):\n",
    "    target.append(dataset.text[n[i]])\n",
    "    reviews_sentences.append(nltk.tokenize.sent_tokenize(target[i]))\n",
    "for i in range(0,5):\n",
    "    required_sentences.append(reviews_sentences[i][0])\n",
    "print(required_sentences)\n",
    "for i in range(0,len(required_sentences)):\n",
    "    required_sentences[i]=nltk_pos_tagging(required_sentences[i])\n",
    "print(required_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
